{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T10:48:17.487410Z",
     "start_time": "2020-07-29T10:48:15.467690Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Util temp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def agg_numeric(df, group_var, df_name):\n",
    "    \"\"\"Aggregates the numeric values in a dataframe. This can\n",
    "    be used to create features for each instance of the grouping variable.\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe):\n",
    "            the dataframe to calculate the statistics on\n",
    "        group_var (string):\n",
    "            the variable by which to group df\n",
    "        df_name (string):\n",
    "            the variable used to rename the columns\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe):\n",
    "            a dataframe with the statistics aggregated for\n",
    "            all numeric columns. Each instance of the grouping variable will have\n",
    "            the statistics (mean, min, max, sum; currently supported) calculated.\n",
    "            The columns are also renamed to keep track of features created.\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != group_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns=col)\n",
    "    group_ids = df[group_var]\n",
    "    numeric_df = df.select_dtypes('number')\n",
    "    numeric_df[group_var] = group_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = [group_var]\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        # Skip the grouping variable\n",
    "        if var != group_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1][:-1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "\n",
    "    agg.columns = columns\n",
    "    return agg\n",
    "\n",
    "def count_categorical(df, group_var, df_name):\n",
    "    \"\"\"Computes counts and normalized counts for each observation\n",
    "    of `group_var` of each unique category in every categorical variable\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        The dataframe to calculate the value counts for.\n",
    "\n",
    "    group_var : string\n",
    "        The variable by which to group the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "\n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with counts and normalized counts of each unique category in every categorical variable\n",
    "        with one row for every unique value of the `group_var`.\n",
    "\n",
    "    \"\"\"\n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('object'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[group_var] = df[group_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n",
    "\n",
    "    column_names = []\n",
    "\n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['count', 'count_norm']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "\n",
    "    categorical.columns = column_names\n",
    "\n",
    "    return categorical\n",
    "\n",
    "def aggregate_client(df, group_vars, df_names):\n",
    "    \"\"\"Aggregate a dataframe with data at the loan level\n",
    "    at the client level\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): data at the loan level\n",
    "        group_vars (list of two strings): grouping variables for the loan\n",
    "        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n",
    "        names (list of two strings): names to call the resulting columns\n",
    "        (example ['cash', 'client'])\n",
    "\n",
    "    Returns:\n",
    "        df_client (dataframe): aggregated numeric stats at the client level.\n",
    "        Each client will have a single row with all the numeric data aggregated\n",
    "    \"\"\"\n",
    "\n",
    "    # Aggregate the numeric columns\n",
    "    df_agg = agg_numeric_new(df, parent_var=group_vars[0], df_name=df_names[0])\n",
    "\n",
    "    # If there are categorical variables\n",
    "    if any(df.dtypes == 'category'):\n",
    "        # Count the categorical columns\n",
    "        df_counts = agg_categorical_new(df, parent_var=group_vars[0], df_name=df_names[0])\n",
    "\n",
    "        # Merge the numeric and categorical\n",
    "        df_by_loan = df_counts.merge(df_agg, on=group_vars[0], how='outer')\n",
    "\n",
    "        # Merge to get the client id in dataframe\n",
    "        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on=group_vars[0], how='left')\n",
    "\n",
    "        # Remove the loan id\n",
    "        df_by_loan = df_by_loan.drop(columns=[group_vars[0]])\n",
    "\n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric_new(df_by_loan, parent_var=group_vars[1], df_name=df_names[1])\n",
    "\n",
    "\n",
    "    # No categorical variables\n",
    "    else:\n",
    "        # Merge to get the client id in dataframe\n",
    "        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on=group_vars[0], how='left')\n",
    "\n",
    "        # Remove the loan id\n",
    "        df_by_loan = df_by_loan.drop(columns=[group_vars[0]])\n",
    "\n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric_new(df_by_loan, parent_var=group_vars[1], df_name=df_names[1])\n",
    "\n",
    "    # Memory management\n",
    "\n",
    "    return df_by_client\n",
    "\n",
    "def convert_types(df, print_info=False):\n",
    "    original_memory = df.memory_usage().sum()\n",
    "\n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "\n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "\n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "\n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "\n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "\n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "\n",
    "    new_memory = df.memory_usage().sum()\n",
    "\n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def agg_numeric_new(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Groups and aggregates the numeric values in a child dataframe\n",
    "    by the parent variable.\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe):\n",
    "            the child dataframe to calculate the statistics on\n",
    "        parent_var (string):\n",
    "            the parent variable used for grouping and aggregating\n",
    "        df_name (string):\n",
    "            the variable used to rename the columns\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe):\n",
    "            a dataframe with the statistics aggregated by the `parent_var` for\n",
    "            all numeric columns. Each observation of the parent variable will have\n",
    "            one row in the dataframe with the parent variable as the index.\n",
    "            The columns are also renamed using the `df_name`. Columns with all duplicate\n",
    "            values are removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != parent_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns=col)\n",
    "\n",
    "    # Only want the numeric variables\n",
    "    parent_ids = df[parent_var].copy()\n",
    "    numeric_df = df.select_dtypes('number').copy()\n",
    "    numeric_df[parent_var] = parent_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = []\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        if var != parent_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "\n",
    "    agg.columns = columns\n",
    "\n",
    "    # Remove the columns with all redundant values\n",
    "    _, idx = np.unique(agg, axis=1, return_index=True)\n",
    "    agg = agg.iloc[:, idx]\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def agg_categorical_new(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Aggregates the categorical features in a child dataframe\n",
    "    for each observation of the parent variable.\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        The dataframe to calculate the value counts for.\n",
    "\n",
    "    parent_var : string\n",
    "        The variable by which to group and aggregate the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "\n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with aggregated statistics for each observation of the parent_var\n",
    "        The columns are also renamed and columns with duplicate values are removed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('category'))\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[parent_var] = df[parent_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n",
    "\n",
    "    column_names = []\n",
    "\n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['sum', 'count', 'mean']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "\n",
    "    categorical.columns = column_names\n",
    "\n",
    "    # Remove duplicate columns by values\n",
    "    _, idx = np.unique(categorical, axis=1, return_index=True)\n",
    "    categorical = categorical.iloc[:, idx]\n",
    "\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T10:53:35.086044Z",
     "start_time": "2020-07-29T10:52:53.577283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bureau and bureau_balance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pande\\.conda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "parentFolder = \"D:/Dataset/home-credit-default-risk/\"\n",
    "print(\"Reading bureau and bureau_balance\")\n",
    "bureau = pd.read_csv(parentFolder+'bureau.csv')\n",
    "bureau_balance = pd.read_csv(parentFolder+'bureau_balance.csv')\n",
    "\n",
    "bureau_counts = count_categorical(bureau, group_var='SK_ID_CURR', df_name='bureau')\n",
    "bureau_counts.head()\n",
    "\n",
    "bureau_agg = agg_numeric(bureau.drop(columns=['SK_ID_BUREAU']), group_var='SK_ID_CURR', df_name='bureau')\n",
    "bureau_agg.head()\n",
    "bureau_agg.fillna(value=0, inplace=True)\n",
    "\n",
    "bureau_balance_counts = count_categorical(bureau_balance, group_var='SK_ID_BUREAU', df_name='bureau_balance')\n",
    "bureau_balance_counts.head()\n",
    "\n",
    "bureau_balance_agg = agg_numeric(bureau_balance, group_var='SK_ID_BUREAU', df_name='bureau_balance')\n",
    "bureau_balance_agg.head()\n",
    "\n",
    "# Dataframe grouped by the loan\n",
    "bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index=True, left_on='SK_ID_BUREAU', how='outer')\n",
    "\n",
    "# Merge to include the SK_ID_CURR\n",
    "bureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "# Aggregate the stats for each client\n",
    "bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns=['SK_ID_BUREAU']), group_var='SK_ID_CURR',\n",
    "                                       df_name='client')\n",
    "\n",
    "\n",
    "\n",
    "df = bureau_counts.merge(bureau_agg, on ='SK_ID_CURR', how='left')\n",
    "df = df.merge(bureau_balance_by_client, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# del bureau_agg, bureau_balance_by_client, bureau_counts, bureau, bureau_by_loan, bureau_balance, bureau_balance_agg, bureau_balance_counts\n",
    "# import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T11:21:34.170391Z",
     "start_time": "2020-07-29T11:21:34.166386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1716428, 17)\n",
      "(817395, 16)\n"
     ]
    }
   ],
   "source": [
    "# app = pd.read_csv(parentFolder+'application_train.csv')\n",
    "print(bureau.shape)\n",
    "print(bureau_balance_counts.shape)\n",
    "# print(app.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T11:29:31.743793Z",
     "start_time": "2020-07-29T11:29:31.716865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bureau_balance_STATUS_0_count</th>\n",
       "      <th>bureau_balance_STATUS_0_count_norm</th>\n",
       "      <th>bureau_balance_STATUS_1_count</th>\n",
       "      <th>bureau_balance_STATUS_1_count_norm</th>\n",
       "      <th>bureau_balance_STATUS_2_count</th>\n",
       "      <th>bureau_balance_STATUS_2_count_norm</th>\n",
       "      <th>bureau_balance_STATUS_3_count</th>\n",
       "      <th>bureau_balance_STATUS_3_count_norm</th>\n",
       "      <th>bureau_balance_STATUS_4_count</th>\n",
       "      <th>bureau_balance_STATUS_4_count_norm</th>\n",
       "      <th>bureau_balance_STATUS_5_count</th>\n",
       "      <th>bureau_balance_STATUS_5_count_norm</th>\n",
       "      <th>bureau_balance_STATUS_C_count</th>\n",
       "      <th>bureau_balance_STATUS_C_count_norm</th>\n",
       "      <th>bureau_balance_STATUS_X_count</th>\n",
       "      <th>bureau_balance_STATUS_X_count_norm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK_ID_BUREAU</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5001709</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>11</td>\n",
       "      <td>0.113402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001710</th>\n",
       "      <td>5</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>30</td>\n",
       "      <td>0.361446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001711</th>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001712</th>\n",
       "      <td>10</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001713</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              bureau_balance_STATUS_0_count  \\\n",
       "SK_ID_BUREAU                                  \n",
       "5001709                                   0   \n",
       "5001710                                   5   \n",
       "5001711                                   3   \n",
       "5001712                                  10   \n",
       "5001713                                   0   \n",
       "\n",
       "              bureau_balance_STATUS_0_count_norm  \\\n",
       "SK_ID_BUREAU                                       \n",
       "5001709                                 0.000000   \n",
       "5001710                                 0.060241   \n",
       "5001711                                 0.750000   \n",
       "5001712                                 0.526316   \n",
       "5001713                                 0.000000   \n",
       "\n",
       "              bureau_balance_STATUS_1_count  \\\n",
       "SK_ID_BUREAU                                  \n",
       "5001709                                   0   \n",
       "5001710                                   0   \n",
       "5001711                                   0   \n",
       "5001712                                   0   \n",
       "5001713                                   0   \n",
       "\n",
       "              bureau_balance_STATUS_1_count_norm  \\\n",
       "SK_ID_BUREAU                                       \n",
       "5001709                                      0.0   \n",
       "5001710                                      0.0   \n",
       "5001711                                      0.0   \n",
       "5001712                                      0.0   \n",
       "5001713                                      0.0   \n",
       "\n",
       "              bureau_balance_STATUS_2_count  \\\n",
       "SK_ID_BUREAU                                  \n",
       "5001709                                   0   \n",
       "5001710                                   0   \n",
       "5001711                                   0   \n",
       "5001712                                   0   \n",
       "5001713                                   0   \n",
       "\n",
       "              bureau_balance_STATUS_2_count_norm  \\\n",
       "SK_ID_BUREAU                                       \n",
       "5001709                                      0.0   \n",
       "5001710                                      0.0   \n",
       "5001711                                      0.0   \n",
       "5001712                                      0.0   \n",
       "5001713                                      0.0   \n",
       "\n",
       "              bureau_balance_STATUS_3_count  \\\n",
       "SK_ID_BUREAU                                  \n",
       "5001709                                   0   \n",
       "5001710                                   0   \n",
       "5001711                                   0   \n",
       "5001712                                   0   \n",
       "5001713                                   0   \n",
       "\n",
       "              bureau_balance_STATUS_3_count_norm  \\\n",
       "SK_ID_BUREAU                                       \n",
       "5001709                                      0.0   \n",
       "5001710                                      0.0   \n",
       "5001711                                      0.0   \n",
       "5001712                                      0.0   \n",
       "5001713                                      0.0   \n",
       "\n",
       "              bureau_balance_STATUS_4_count  \\\n",
       "SK_ID_BUREAU                                  \n",
       "5001709                                   0   \n",
       "5001710                                   0   \n",
       "5001711                                   0   \n",
       "5001712                                   0   \n",
       "5001713                                   0   \n",
       "\n",
       "              bureau_balance_STATUS_4_count_norm  \\\n",
       "SK_ID_BUREAU                                       \n",
       "5001709                                      0.0   \n",
       "5001710                                      0.0   \n",
       "5001711                                      0.0   \n",
       "5001712                                      0.0   \n",
       "5001713                                      0.0   \n",
       "\n",
       "              bureau_balance_STATUS_5_count  \\\n",
       "SK_ID_BUREAU                                  \n",
       "5001709                                   0   \n",
       "5001710                                   0   \n",
       "5001711                                   0   \n",
       "5001712                                   0   \n",
       "5001713                                   0   \n",
       "\n",
       "              bureau_balance_STATUS_5_count_norm  \\\n",
       "SK_ID_BUREAU                                       \n",
       "5001709                                      0.0   \n",
       "5001710                                      0.0   \n",
       "5001711                                      0.0   \n",
       "5001712                                      0.0   \n",
       "5001713                                      0.0   \n",
       "\n",
       "              bureau_balance_STATUS_C_count  \\\n",
       "SK_ID_BUREAU                                  \n",
       "5001709                                  86   \n",
       "5001710                                  48   \n",
       "5001711                                   0   \n",
       "5001712                                   9   \n",
       "5001713                                   0   \n",
       "\n",
       "              bureau_balance_STATUS_C_count_norm  \\\n",
       "SK_ID_BUREAU                                       \n",
       "5001709                                 0.886598   \n",
       "5001710                                 0.578313   \n",
       "5001711                                 0.000000   \n",
       "5001712                                 0.473684   \n",
       "5001713                                 0.000000   \n",
       "\n",
       "              bureau_balance_STATUS_X_count  \\\n",
       "SK_ID_BUREAU                                  \n",
       "5001709                                  11   \n",
       "5001710                                  30   \n",
       "5001711                                   1   \n",
       "5001712                                   0   \n",
       "5001713                                  22   \n",
       "\n",
       "              bureau_balance_STATUS_X_count_norm  \n",
       "SK_ID_BUREAU                                      \n",
       "5001709                                 0.113402  \n",
       "5001710                                 0.361446  \n",
       "5001711                                 0.250000  \n",
       "5001712                                 0.000000  \n",
       "5001713                                 1.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_balance_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T11:19:04.380222Z",
     "start_time": "2020-07-29T11:19:04.338303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305811\n"
     ]
    }
   ],
   "source": [
    "print(len(bureau['SK_ID_CURR'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T10:55:31.288755Z",
     "start_time": "2020-07-29T10:55:31.214561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bureau_balance_STATUS_0_count         0\n",
       "bureau_balance_STATUS_0_count_norm    0\n",
       "bureau_balance_STATUS_1_count         0\n",
       "bureau_balance_STATUS_1_count_norm    0\n",
       "bureau_balance_STATUS_2_count         0\n",
       "bureau_balance_STATUS_2_count_norm    0\n",
       "bureau_balance_STATUS_3_count         0\n",
       "bureau_balance_STATUS_3_count_norm    0\n",
       "bureau_balance_STATUS_4_count         0\n",
       "bureau_balance_STATUS_4_count_norm    0\n",
       "bureau_balance_STATUS_5_count         0\n",
       "bureau_balance_STATUS_5_count_norm    0\n",
       "bureau_balance_STATUS_C_count         0\n",
       "bureau_balance_STATUS_C_count_norm    0\n",
       "bureau_balance_STATUS_X_count         0\n",
       "bureau_balance_STATUS_X_count_norm    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_balance_counts.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T10:54:13.943671Z",
     "start_time": "2020-07-29T10:54:13.694402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SK_ID_CURR                                              0\n",
       "bureau_CREDIT_ACTIVE_Active_count                       0\n",
       "bureau_CREDIT_ACTIVE_Active_count_norm                  0\n",
       "bureau_CREDIT_ACTIVE_Bad debt_count                     0\n",
       "bureau_CREDIT_ACTIVE_Bad debt_count_norm                0\n",
       "                                                    ...  \n",
       "client_bureau_balance_STATUS_X_count_norm_count         0\n",
       "client_bureau_balance_STATUS_X_count_norm_mean     171269\n",
       "client_bureau_balance_STATUS_X_count_norm_max      171269\n",
       "client_bureau_balance_STATUS_X_count_norm_min      171269\n",
       "client_bureau_balance_STATUS_X_count_norm_sum           0\n",
       "Length: 212, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"Reading POS_CASH_BALANCE\")\n",
    "cash = pd.read_csv(parentFolder+'POS_CASH_balance.csv')\n",
    "cash = convert_types(cash, print_info=True)\n",
    "cash_by_client = aggregate_client(cash, group_vars=['SK_ID_PREV', 'SK_ID_CURR'], df_names=['cash', 'client'])\n",
    "\n",
    "# app_train_poly = app_train_poly.merge(cash_by_client, on='SK_ID_CURR', how='left')\n",
    "df = df.merge(cash_by_client, on='SK_ID_CURR', how='left')\n",
    "del cash, cash_by_client\n",
    "gc.collect();\n",
    "\n",
    "\n",
    "print(\"Reading Credit_card_balance\")\n",
    "credit = pd.read_csv(parentFolder+'credit_card_balance.csv')\n",
    "credit = convert_types(credit, print_info=True)\n",
    "credit_by_client = aggregate_client(credit, group_vars=['SK_ID_PREV', 'SK_ID_CURR'], df_names=['credit', 'client'])\n",
    "\n",
    "# app_train_poly = app_train_poly.merge(credit_by_client, on='SK_ID_CURR', how='left')\n",
    "df = df.merge(credit_by_client, on='SK_ID_CURR', how='left')\n",
    "del credit, credit_by_client; gc.collect()\n",
    "\n",
    "\n",
    "print(\"Reading installment_payments\")\n",
    "installments = pd.read_csv(parentFolder+'installments_payments.csv')\n",
    "installments = convert_types(installments, print_info=True)\n",
    "\n",
    "installments_by_client = aggregate_client(installments, group_vars=['SK_ID_PREV', 'SK_ID_CURR'],\n",
    "                                          df_names=['installments', 'client'])\n",
    "\n",
    "# app_train_poly = app_train_poly.merge(installments_by_client, on='SK_ID_CURR', how='left')\n",
    "df = df.merge(installments_by_client, on='SK_ID_CURR', how='left')\n",
    "del installments_by_client, installments\n",
    "gc.collect()\n",
    "\n",
    "# cols = df.columns\n",
    "\n",
    "# print(\"Before Imputer shape: \", df.shape)\n",
    "# ## Impute\n",
    "# imputer = SimpleImputer(strategy='median')\n",
    "# df = imputer.fit_transform(df)\n",
    "# print(\"After Imputer shape: \", df.shape)\n",
    "\n",
    "df_final = pd.DataFrame(data=df, columns=cols)\n",
    "del(df); gc.collect()\n",
    "\n",
    "\n",
    "df_final.to_csv(\"./Res/PastMerged1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T03:21:44.022955Z",
     "start_time": "2020-07-30T03:21:44.014477Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "\n",
    "parent = 'D:/Projects/Spyder/LoanDef/Res/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:28:08.134530Z",
     "start_time": "2020-07-29T12:27:23.215852Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(parent+'final1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:28:53.717014Z",
     "start_time": "2020-07-29T12:28:53.662165Z"
    }
   },
   "outputs": [],
   "source": [
    "label_path = parent+'training_lbl1.pkl'\n",
    "label = pickle.load(open(label_path,'rb'))\n",
    "y = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:34:29.856831Z",
     "start_time": "2020-07-29T12:34:25.126775Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0', 'SK_ID_CURR'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:42:12.634480Z",
     "start_time": "2020-07-29T12:42:12.629491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24825"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:42:51.343523Z",
     "start_time": "2020-07-29T12:42:51.338536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24825*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:45:00.716212Z",
     "start_time": "2020-07-29T12:45:00.712189Z"
    }
   },
   "outputs": [],
   "source": [
    "rand_shuffle = np.arange(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:45:26.006950Z",
     "start_time": "2020-07-29T12:45:25.992986Z"
    }
   },
   "outputs": [],
   "source": [
    "x = df[rand_shuffle]\n",
    "y= y[rand_shuffle]\n",
    "all_pos = np.where(y == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:47:04.945692Z",
     "start_time": "2020-07-29T12:47:04.939713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,     26,     40, ..., 307481, 307489, 307509], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.where(y == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:52:13.448494Z",
     "start_time": "2020-07-29T12:52:05.310465Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=101, test_size=.20, stratify=y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:52:33.400962Z",
     "start_time": "2020-07-29T12:52:33.390019Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:37:30.553742Z",
     "start_time": "2020-07-29T12:37:30.550750Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:54:09.182047Z",
     "start_time": "2020-07-29T12:54:09.168084Z"
    }
   },
   "outputs": [],
   "source": [
    "solver='lbfgs'\n",
    "alpha=1e-4\n",
    "hls=(150, 100, 50)\n",
    "rand_state=229\n",
    "clf_nn = MLPClassifier(\n",
    "        solver=solver, alpha=alpha, hidden_layer_sizes=hls,\n",
    "        random_state=rand_state, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:19:05.714366Z",
     "start_time": "2020-07-29T12:54:09.923658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pande\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(150, 100, 50), random_state=229,\n",
       "              solver='lbfgs', verbose=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T14:37:11.292595Z",
     "start_time": "2020-07-29T14:37:09.915773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     56538\n",
      "           1       0.39      0.03      0.06      4965\n",
      "\n",
      "    accuracy                           0.92     61503\n",
      "   macro avg       0.66      0.51      0.51     61503\n",
      "weighted avg       0.88      0.92      0.88     61503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test.reshape(-1,1), clf_nn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T14:44:38.171065Z",
     "start_time": "2020-07-29T14:40:16.520006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='Balanced', max_iter=1000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000 )\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T03:15:59.644582Z",
     "start_time": "2020-07-30T03:15:59.184973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56504    34]\n",
      " [ 4916    49]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     56538\n",
      "           1       0.59      0.01      0.02      4965\n",
      "\n",
      "    accuracy                           0.92     61503\n",
      "   macro avg       0.76      0.50      0.49     61503\n",
      "weighted avg       0.89      0.92      0.88     61503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test.reshape(-1,1), lr.predict(X_test)))\n",
    "print(classification_report(y_test.reshape(-1,1), lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-30T03:26:15.316Z"
    }
   },
   "outputs": [],
   "source": [
    "nm = NearMiss()\n",
    "X_res, y_res = nm.fit_resample(df, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T03:26:07.635823Z",
     "start_time": "2020-07-30T03:26:07.617662Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T15:15:23.329122Z",
     "start_time": "2020-07-30T15:15:23.307068Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e4c3ef879cde>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e4c3ef879cde>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
